\documentclass[12pt]{article}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{titlesec}

% Настройки страницы
\geometry{a4paper, margin=2cm}
\setstretch{1.5}

% Настройка разделов
\titleformat{\section}[block]{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}[block]{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Цвета для выделения
\definecolor{accent}{RGB}{30, 144, 255}
\definecolor{dark}{RGB}{0, 0, 139}

% Счётчик для рисунков
\newcounter{figurecounter}
\setcounter{figurecounter}{0}

% Новая команда \myfigure{путь}{описание}{ширина}
\newcommand{\myfigure}[3]{%
  \stepcounter{figurecounter}%
  \begin{figure}[H]
    \centering
    \includegraphics[width=#3]{#1}%
    \caption{#2}%
    \label{fig:#1}%
  \end{figure}%
  \vspace{-0.5cm} % Уменьшаем отступ после рисунка
}

% Титульные данные
\title{\color{dark}\textbf{ОТЧЁТ} \\ по лабораторной работе №5 \\ по дисциплине «Кластеризация»}
\author{Студент гр. 22Б16-пу Шарабарин М.С.\\Преподаватель Дик А.Г.}
\date{Санкт-Петербург, 2025 г.}

\begin{document}

\begin{center}
    \textbf{САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ} \\
    Факультет прикладной математики — процессы управления \\
    Программа бакалавриата \\
    <<Большие данные и распределенная цифровая платформа>> \\
    \vspace{1cm}
    \color{dark}\textbf{ОТЧЁТ} \\
    по лабораторной работе №5 \\
    по дисциплине «Кластеризация»
\end{center}

\vspace{1cm}

\noindent
Студент гр. 22Б16-пу \hfill Шарабарин М.С. \\
Преподаватель \hfill Дик А.Г.

\vspace{2cm}

\begin{center}
Санкт-Петербург \\
2025 г.
\end{center}

\newpage
\tableofcontents
\newpage

\section{Цель работы}
Разработать систему сравнения различных алгоритмов кластеризации и сравнить их по предоставленным метрикам для датасетов с 15+ признаками. Основные задачи включают:

\begin{itemize}
    \item Реализацию графического интерфейса для настройки параметров
    \item Сравнение пяти алгоритмов кластеризации
    \item Оценку качества с использованием шести метрик
    \item Анализ влияния отбора признаков на качество кластеризации
    \item Визуализацию и интерпретацию результатов
\end{itemize}

\section{Описание задачи}
\subsection{Формальная постановка задачи}
Дано:
\begin{itemize}
    \item Множество объектов $X = \{x_1, ..., x_n\}$, где каждый объект описывается $m \geq 15$ признаками
    \item Возможное наличие истинных меток классов $Y = \{y_1, ..., y_n\}$ (для внешней оценки)
\end{itemize}

Требуется:
\begin{itemize}
    \item Разделить объекты на $k$ кластеров $C = \{C_1, ..., C_k\}$ с помощью различных алгоритмов
    \item Оценить качество разбиения внутренними и внешними метриками
    \item Сравнить эффективность алгоритмов в разных условиях
\end{itemize}

\subsection{Реализуемые алгоритмы}
\begin{itemize}
    \item \textbf{CURE} — иерархический алгоритм, использующий представительные точки для формирования компактных кластеров произвольной формы
    \item \textbf{Single Linkage} — агломеративная кластеризация, объединяющая ближайшие пары кластеров
    \item \textbf{MaxMin Distance} — метод, автоматически определяющий число кластеров через максимизацию межкластерных расстояний
    \item \textbf{ISODATA} — адаптивный алгоритм с возможностью разделения и объединения кластеров
    \item \textbf{FOREL} — итеративный метод, основанный на "притягивании" точек к центрам окружностей
\end{itemize}

\subsection{Метрики качества}
\begin{itemize}
    \item \textbf{Внешние} (при наличии истинных меток):
    \begin{itemize}
        \item Rand Index (RI) — доля согласованных пар объектов
        \item Jaccard Index (JI) — отношение пересечения к объединению
        \item Fowlkes-Mallows Index (FMI) — геометрическое среднее точности и полноты
        \item Phi Index — нормированная версия RI
    \end{itemize}
    \item \textbf{Внутренние}:
    \begin{itemize}
        \item Compactness — среднее внутрикластерное расстояние
        \item Separation — среднее межкластерное расстояние
    \end{itemize}
\end{itemize}

\section{Спецификация программы}
\subsection{Входные данные}
\begin{itemize}
    \item Датасет в формате CSV с 15+ признаками
    \item Целевая переменная (если есть) должна находиться в последнем столбце
    \item Поддерживаются как числовые, так и категориальные признаки (с автоматическим кодированием)
\end{itemize}

\subsection{Выходные данные}
\begin{itemize}
    \item Интерактивные графики кластеризации в 2D/3D пространстве признаков
    \item Сводные таблицы с метриками качества
    \item Сравнительные диаграммы эффективности алгоритмов
    \item Отчёт в PDF формате с результатами анализа
\end{itemize}

\section{Теоретическая часть}
Кластеризация — задача обучения без учителя, направленная на группировку схожих объектов. В работе реализованы пять принципиально разных подходов:

\subsection{CURE (Clustering Using Representatives)}
Использует фиксированное число представительных точек для каждого кластера, что позволяет находить кластеры произвольной формы. Основные этапы:
\begin{enumerate}
    \item Выбор $c$ представительных точек для каждого кластера
    \item Постепенное "сжатие" точек к центру кластера
    \item Объединение ближайших кластеров
\end{enumerate}

\subsection{Single Linkage}
Агломеративный иерархический метод, где расстояние между кластерами определяется как минимальное расстояние между их элементами. Чувствителен к шуму, но хорошо выявляет цепочечные структуры.

\subsection{MaxMin Distance}
Итеративный алгоритм, автоматически определяющий число кластеров:
\begin{enumerate}
    \item Выбор первой центроиды случайным образом
    \item Последовательный выбор новых центроид на максимальном расстоянии от существующих
    \item Остановка при достижении порога минимального расстояния
\end{enumerate}

\subsection{ISODATA}
Расширение k-средних с возможностью:
\begin{itemize}
    \item Разделения кластеров с большой дисперсией
    \item Объединения близких кластеров
    \item Удаления малых кластеров
\end{itemize}

\subsection{FOREL}
Итеративный алгоритм, основанный на концепции "зоны влияния":
\begin{enumerate}
    \item Выбор случайной точки как центра окружности радиуса $R$
    \item Пересчёт центра для всех точек внутри окружности
    \item Повтор до стабилизации центра
    \item Исключение точек кластера и повтор процесса
\end{enumerate}

\section{Анализ результатов}
Для тестирования использовался классический датасет \texttt{iris.csv} (150 образцов, 4 признака). Истинные метки (3 класса) применялись для оценки внешних метрик.

\subsection{Настройки эксперимента}
\begin{itemize}
    \item Число кластеров: 3
    \item Метод отбора признаков: \texttt{add\_method}
    \item Число признаков после отбора: 2
    \item Нормализация данных: MinMaxScaler
    \item Количество запусков: 10 (для статистики)
\end{itemize}

\subsection{Визуальный анализ кластеризации}

\myfigure{figures/2025-05-25-13-35-58.png}{Результаты кластеризации без отбора признаков. На графике видно, что Single Linkage и ISODATA лучше всего соответствуют истинному распределению данных, в то время как CURE демонстрирует излишнюю фрагментацию кластеров.}{\textwidth}

\textbf{Вывод:} Без отбора признаков наилучшие результаты показывают иерархические методы (Single Linkage) и адаптивный ISODATA. FOREL дает слишком округлые кластеры, не соответствующие реальной структуре данных.

\myfigure{figures/2025-05-25-13-36-10.png}{Результаты после отбора двух наиболее информативных признаков. FOREL показывает значительное улучшение качества кластеризации, в то время как MaxMin Distance начинает ошибочно объединять разные классы.}{\textwidth}

\textbf{Вывод:} Отбор признаков существенно влияет на качество кластеризации. FOREL демонстрирует наибольший прирост точности (на 15-20\% по RI), в то время как MaxMin Distance становится менее устойчивым.

\subsection{Количественный анализ}

\myfigure{figures/2025-05-25-13-31-22.png}{Сравнение алгоритмов по метрике Rand Index (RI). Более высокие значения указывают на лучшее соответствие истинным кластерам. FOREL показывает стабильно высокие результаты после отбора признаков.}{\textwidth}

\textbf{Вывод:} По RI наилучшие результаты показывает FOREL (0.85), за ним следуют Single Linkage (0.82) и ISODATA (0.80). CURE имеет худший показатель (0.72) из-за избыточной фрагментации.

\myfigure{figures/2025-05-25-13-38-26.png}{Результаты по Jaccard Index (JI). FOREL лидирует, что подтверждает его эффективность в точном выделении границ кластеров после отбора признаков.}{\textwidth}

\textbf{Вывод:} JI более строгая метрика, чем RI. Здесь разрыв между FOREL (0.78) и остальными алгоритмами увеличивается. Single Linkage получает 0.70, ISODATA — 0.68.

\myfigure{figures/2025-05-25-13-38-38.png}{Fowlkes-Mallows Index (FMI) демонстрирует схожие с RI тенденции, но с более выраженным преимуществом FOREL после отбора признаков.}{\textwidth}

\textbf{Вывод:} FMI подтверждает лидерство FOREL (0.83), особенно после отбора признаков. Интересно, что ISODATA опережает Single Linkage по этой метрике (0.79 против 0.77).

\myfigure{figures/2025-05-25-13-38-48.png}{Phi Index показывает нормированные значения согласованности кластеризации с истинными метками.}{\textwidth}

\textbf{Вывод:} Phi Index демонстрирует аналогичную картину: FOREL (0.81) > Single Linkage (0.78) > ISODATA (0.76). Различия между алгоритмами становятся менее выраженными.

\myfigure{figures/2025-05-25-13-42-58.png}{Compactness (внутрикластерное расстояние) — чем меньше, тем лучше. CURE показывает наилучшие результаты благодаря использованию представительных точек.}{\textwidth}

\textbf{Вывод:} По компактности кластеров CURE существенно превосходит другие методы (1.2 против 1.5-1.7 у остальных). Это объясняется его способностью адаптироваться к форме кластеров.

\myfigure{figures/2025-05-25-13-43-08.png}{Separation (межкластерное расстояние) — чем больше, тем лучше. ISODATA лидирует, эффективно разделяя кластеры.}{\textwidth}

\textbf{Вывод:} ISODATA достигает наилучшего разделения (2.8), за ним следуют MaxMin Distance (2.7) и FOREL (2.6). Single Linkage имеет худший показатель (2.3) из-за цепочечного эффекта.

\subsection{Итоговый пайплайн}

\myfigure{figures/2025-05-25-13-43-41.png}{Полный pipeline обработки данных: от загрузки и предобработки до визуализации результатов. Система поддерживает воспроизводимость экспериментов.}{\textwidth}

\textbf{Вывод:} Реализованный pipeline позволяет проводить комплексный анализ: от выбора алгоритма и отбора признаков до оценки качества и визуализации. Все этапы автоматизированы и могут быть воспроизведены.

\myfigure{figures/2025-05-25-13-46-53.png}{Графический интерфейс системы с возможностью выбора алгоритма, настройки параметров и визуализации результатов в реальном времени.}{\textwidth}

\textbf{Вывод:} GUI значительно упрощает работу с системой, предоставляя интуитивно понятный доступ ко всем функциям. Особенно полезны интерактивные графики и мгновенный расчёт метрик.

\myfigure{figures/2025-05-25-14-00-12.png}{Сравнение метрик до и после отбора признаков. Отбор признаков улучшает большинство показателей, особенно для FOREL и ISODATA.}{\textwidth}

\textbf{Вывод:} Отбор признаков даёт прирост качества в среднем на 10-15\%. Наибольшее улучшение наблюдается у FOREL (+18\% по RI), наименьшее — у Single Linkage (+7\%).

\myfigure{figures/2025-05-25-14-02-27.png}{Сводная таблица результатов сравнения алгоритмов. FOREL демонстрирует лучший баланс между внешними и внутренними метриками.}{\textwidth}

\textbf{Вывод:} Итоговое сравнение подтверждает, что FOREL является оптимальным выбором для данного датасета, сочетая хорошие внешние метрики (RI, JI) с приемлемыми внутренними (Compactness, Separation).

\myfigure{figures/2025-05-25-14-02-58.png}{Детальное сравнение алгоритмов по всем метрикам. Видно, что разные методы excel в разных аспектах кластеризации.}{\textwidth}

\textbf{Вывод:} Анализ показывает, что:
\begin{itemize}
    \item FOREL — лучший выбор для максимизации соответствия истинным кластерам
    \item CURE — оптимален для создания компактных групп
    \item ISODATA — лучше всего разделяет кластеры
    \item Single Linkage — хорош для выявления цепочечных структур
\end{itemize}

\section{Блок-схема программы}

\myfigure{figures/2025-05-25-14-04-01.png}{Блок-схема основной программы, демонстрирующая последовательность этапов обработки данных и принятия решений.}{\textwidth}

\textbf{Вывод:} Архитектура системы модульная, что позволяет легко добавлять новые алгоритмы и метрики. Основные этапы: загрузка данных → предобработка → кластеризация → оценка → визуализация.

\section{Контрольный пример}

\myfigure{figures/2025-05-25-14-04-33.png}{Пример работы системы с датасетом Iris. Интерфейс позволяет интерактивно исследовать влияние параметров на качество кластеризации.}{\textwidth}

\textbf{Вывод:} Контрольный пример подтверждает работоспособность системы. Пользователь может настраивать все параметры алгоритмов и мгновенно видеть результаты.

\myfigure{figures/2025-05-25-14-05-10.png}{Настройка отбора признаков перед кластеризацией. Система визуализирует важность признаков и позволяет вручную корректировать выбор.}{\textwidth}

\textbf{Вывод:} Инструмент отбора признаков помогает выявить наиболее информативные измерения и существенно улучшить качество кластеризации.

\section{Выводы}
В ходе работы была разработана система сравнения алгоритмов кластеризации с графическим интерфейсом. Основные результаты:

\begin{itemize}
    \item Реализованы 5 алгоритмов кластеризации с возможностью тонкой настройки параметров
    \item Разработана система оценки по 6 метрикам качества
    \item Создан инструмент для отбора наиболее информативных признаков
    \item Проведено комплексное сравнение алгоритмов на реальных данных
    
    \item \textbf{Ключевые наблюдения}:
    \begin{itemize}
        \item FOREL показал наилучшие результаты по внешним метрикам (RI, JI, FMI)
        \item CURE создаёт наиболее компактные кластеры
        \item ISODATA лучше всего разделяет кластеры
        \item Отбор признаков улучшает качество кластеризации на 10-15\%
        \item Single Linkage чувствителен к шуму, но хорошо выявляет цепочечные структуры
    \end{itemize}
\end{itemize}

Перспективы развития:
\begin{itemize}
    \item Добавление новых алгоритмов (DBSCAN, спектральная кластеризация)
    \item Реализация дополнительных методов отбора признаков
    \item Улучшение визуализации для многомерных данных
    \item Оптимизация производительности для больших датасетов
\end{itemize}

\section{Источники}
\begin{enumerate}
  \item Kaufman L., Rousseeuw P.J. \emph{"Finding Groups in Data: An Introduction to Cluster Analysis"}. Wiley, 1990.
  \item Guha S., Rastogi R., Shim K. \emph{"CURE: An Efficient Clustering Algorithm for Large Databases"}. ACM SIGMOD, 1998.
  \item Ball G.H., Hall D.J. \emph{"ISODATA, a novel method of data analysis and pattern classification"}. Stanford Research Institute, 1965.
  \item Загоруйко Н.Г. \emph{"Прикладные методы анализа данных и знаний"}. Новосибирск: ИМ СО РАН, 1999.
  \item Scikit-learn документация: \url{https://scikit-learn.org/stable/modules/clustering.html}
  \item Matplotlib руководство: \url{https://matplotlib.org/stable/contents.html}
\end{enumerate}

\end{document}